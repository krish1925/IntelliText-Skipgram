{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/krishpatel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/krishpatel/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/krishpatel/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/krishpatel/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /Users/krishpatel/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Basic experimentation with nltk\n",
    "# Install nltk using pip before usage\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('treebank')\n",
    "from nltk.translate import bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'basic', 'test', 'sentence', 'to', 'see', 'how', 'nltk', 'works', '.']\n",
      "(S\n",
      "  This/DT\n",
      "  is/VBZ\n",
      "  a/DT\n",
      "  basic/JJ\n",
      "  test/NN\n",
      "  sentence/NN\n",
      "  to/TO\n",
      "  see/VB\n",
      "  how/WRB\n",
      "  nltk/JJ\n",
      "  works/NNS\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# Testing out nltk's tokenization\n",
    "\n",
    "Test_Sentence = \"This is a basic test sentence to see how nltk works.\"\n",
    "tokens = nltk.word_tokenize(Test_Sentence)\n",
    "print(tokens)\n",
    "\n",
    "#testing out and printing tokens\n",
    "entities = nltk.chunk.ne_chunk(nltk.pos_tag(tokens))\n",
    "print(entities)\n",
    "\n",
    "# doesn't work because mac doesn't support tkinter\n",
    "# from nltk.corpus import treebank\n",
    "# t = treebank.parsed_sents('wsj_0001.mrg')[0]\n",
    "# t.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11764705882352941\n"
     ]
    }
   ],
   "source": [
    "# Testing out Bleu Scoring\n",
    "sentence1 = \"Two string of words which are the same but don't have a single same word\"\n",
    "sentence2 = \"Despite their stark linguistic differences, both sentences convey the same underlying message with distinct vocabulary and structure.\"\n",
    "bleu_score = bleu([sentence1.split()], sentence2.split(),  (1,),)\n",
    "print(bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.401, 'pos': 0.599, 'compound': 0.8165}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/krishpatel/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Testing out nltk's sentimental analysis\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentence1 = \"This is very good! I am really happy.\"\n",
    "tokens = nltk.word_tokenize(sentence1)\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "ss = sid.polarity_scores(sentence1)\n",
    "print(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a paragraph.', 'This is because there are multiple sentences in this text.', 'However, paragraphs are not separated with newline characters.', \"Isn't that obvious?\"]\n",
      "Sentence:  This is a paragraph.\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "\n",
      "Sentence:  This is because there are multiple sentences in this text.\n",
      "{'neg': 0.0, 'neu': 0.882, 'pos': 0.118, 'compound': 0.0516}\n",
      "\n",
      "\n",
      "Sentence:  However, paragraphs are not separated with newline characters.\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "\n",
      "Sentence:  Isn't that obvious?\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Converting paragraphs into sentences\n",
    "\n",
    "paragraph = \"This is a paragraph. This is because there are multiple sentences in this text. However, paragraphs are not separated with newline characters. Isn't that obvious?\"\n",
    "sentence_list = nltk.sent_tokenize(paragraph)\n",
    "print(sentence_list)\n",
    "for i in range(len(sentence_list)):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    ss = sid.polarity_scores(sentence_list[i])\n",
    "    print(\"Sentence: \", sentence_list[i])\n",
    "    print(ss)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', ...]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/krishpatel/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#accesing nltk's corpora\n",
    "nltk.download(\"gutenberg\")\n",
    "\n",
    "from nltk.corpus import gutenberg\n",
    "print(\"hello\")\n",
    "print(gutenberg.words())\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=17011, vector_size=100, alpha=0.025>\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Using gensim's word2vec \n",
    "import gensim\n",
    "\n",
    "#training using nltk's corpus \n",
    "training_data = gutenberg.sents()[:100000]\n",
    "models = gensim.models.Word2Vec(training_data)\n",
    "print(models)\n",
    "print(models.sorted_vocab)\n",
    "\n",
    "#saving this model as gutenberg_word2vec\n",
    "models.save('Gutenberg_Model')\n",
    "#loading the model \n",
    "gutenberg_model = gensim.models.Word2Vec.load('Gutenberg_Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product of the two vectors are:  1.2413924\n",
      "Cosine of the two vectors are:  1.0\n",
      "There is a  52.34339108294756  percentage similarity between the two words.\n",
      "The similarity between two words =  0.52343386\n"
     ]
    }
   ],
   "source": [
    "# Using this gutenberg model \n",
    "word1_vector  = gutenberg_model.wv['print']\n",
    "# print(word1_vector)\n",
    "word2_vector = gutenberg_model.wv['print']\n",
    "dot_prod = np.dot(word1_vector, word2_vector)\n",
    "magnitude_product = np.dot(word1_vector, word1_vector)**0.5* np.dot(word2_vector, word2_vector)**0.5\n",
    "print(\"Dot product of the two vectors are: \", dot_prod)\n",
    "print(\"Cosine of the two vectors are: \", dot_prod/magnitude_product)\n",
    "\n",
    "def convert_word2vec(word1):\n",
    "    wordvector = gutenberg_model.wv[word1]\n",
    "    return wordvector\n",
    "\n",
    "def word_similarity(word1vector, word2vector):\n",
    "    dot_prod = np.dot(word1vector, word2vector)\n",
    "    magnitude_product=  np.dot(word2vector, word2vector)**0.5 * np.dot(word1vector, word1vector)**0.5\n",
    "    similarity = dot_prod/magnitude_product\n",
    "    return similarity\n",
    "\n",
    "input1 = input(\"Enter the first word: \")\n",
    "input2 = input(\"Enter the second word: \")\n",
    "print(\"There is a \", word_similarity(convert_word2vec(input1), convert_word2vec(input2))*100,\" percentage similarity between the two words.\" )\n",
    "\n",
    "\n",
    "#This is equivalent to the same thing:\n",
    "print(\"The similarity between two words = \", gutenberg_model.wv.similarity(input1, input2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package word2vec_sample to\n",
      "[nltk_data]     /Users/krishpatel/nltk_data...\n",
      "[nltk_data]   Unzipping models/word2vec_sample.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.data import find\n",
    "nltk.download(\"word2vec_sample\")\n",
    "word2vec_samplee = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_samplee, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "[('universities', 0.7003918290138245), ('faculty', 0.6780906915664673), ('undergraduate', 0.6587095856666565), ('campus', 0.6434988379478455), ('college', 0.638526976108551), ('academic', 0.6317198872566223), ('professors', 0.6298646926879883), ('undergraduates', 0.6149812936782837), ('University', 0.6139305233955383), ('student', 0.600540041923523)]\n",
      "[('Flashed', 0.19817940890789032), ('9N', 0.16438086330890656), ('Neversink', 0.1642790585756302), ('Blanched', 0.15624172985553741), ('Grapefruit', 0.15487904846668243), ('domi', 0.1502796858549118), ('pulverizing', 0.14868904650211334), ('Nosebleed', 0.14782992005348206), ('Tempter', 0.14712963998317719), ('thickeners', 0.14493298530578613)]\n"
     ]
    }
   ],
   "source": [
    "len(model)\n",
    "print(len(model['university']))\n",
    "#finding similar words\n",
    "print(model.most_similar(positive=['university'], topn=10 ))\n",
    "#finding the most dis-similar words\n",
    "print(model.most_similar(negative=['university'], topn=10 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
