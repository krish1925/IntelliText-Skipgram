{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia Content Length: 1906253\n",
      "Training epoch 1/100\n",
      "Training epoch 2/100\n",
      "Training epoch 3/100\n",
      "Training epoch 4/100\n",
      "Training epoch 5/100\n",
      "Training epoch 6/100\n",
      "Training epoch 7/100\n",
      "Training epoch 8/100\n",
      "Training epoch 9/100\n",
      "Training epoch 10/100\n",
      "Training epoch 11/100\n",
      "Training epoch 12/100\n",
      "Training epoch 13/100\n",
      "Training epoch 14/100\n",
      "Training epoch 15/100\n",
      "Training epoch 16/100\n",
      "Training epoch 17/100\n",
      "Training epoch 18/100\n",
      "Training epoch 19/100\n",
      "Training epoch 20/100\n",
      "Training epoch 21/100\n",
      "Training epoch 22/100\n",
      "Training epoch 23/100\n",
      "Training epoch 24/100\n",
      "Training epoch 25/100\n",
      "Training epoch 26/100\n",
      "Training epoch 27/100\n",
      "Training epoch 28/100\n",
      "Training epoch 29/100\n",
      "Training epoch 30/100\n",
      "Training epoch 31/100\n",
      "Training epoch 32/100\n",
      "Training epoch 33/100\n",
      "Training epoch 34/100\n",
      "Training epoch 35/100\n",
      "Training epoch 36/100\n",
      "Training epoch 37/100\n",
      "Training epoch 38/100\n",
      "Training epoch 39/100\n",
      "Training epoch 40/100\n",
      "Training epoch 41/100\n",
      "Training epoch 42/100\n",
      "Training epoch 43/100\n",
      "Training epoch 44/100\n",
      "Training epoch 45/100\n",
      "Training epoch 46/100\n",
      "Training epoch 47/100\n",
      "Training epoch 48/100\n",
      "Training epoch 49/100\n",
      "Training epoch 50/100\n",
      "Training epoch 51/100\n",
      "Training epoch 52/100\n",
      "Training epoch 53/100\n",
      "Training epoch 54/100\n",
      "Training epoch 55/100\n",
      "Training epoch 56/100\n",
      "Training epoch 57/100\n",
      "Training epoch 58/100\n",
      "Training epoch 59/100\n",
      "Training epoch 60/100\n",
      "Training epoch 61/100\n",
      "Training epoch 62/100\n",
      "Training epoch 63/100\n",
      "Training epoch 64/100\n",
      "Training epoch 65/100\n",
      "Training epoch 66/100\n",
      "Training epoch 67/100\n",
      "Training epoch 68/100\n",
      "Training epoch 69/100\n",
      "Training epoch 70/100\n",
      "Training epoch 71/100\n",
      "Training epoch 72/100\n",
      "Training epoch 73/100\n",
      "Training epoch 74/100\n",
      "Training epoch 75/100\n",
      "Training epoch 76/100\n",
      "Training epoch 77/100\n",
      "Training epoch 78/100\n",
      "Training epoch 79/100\n",
      "Training epoch 80/100\n",
      "Training epoch 81/100\n",
      "Training epoch 82/100\n",
      "Training epoch 83/100\n",
      "Training epoch 84/100\n",
      "Training epoch 85/100\n",
      "Training epoch 86/100\n",
      "Training epoch 87/100\n",
      "Training epoch 88/100\n",
      "Training epoch 89/100\n",
      "Training epoch 90/100\n",
      "Training epoch 91/100\n",
      "Training epoch 92/100\n",
      "Training epoch 93/100\n",
      "Training epoch 94/100\n",
      "Training epoch 95/100\n",
      "Training epoch 96/100\n",
      "Training epoch 97/100\n",
      "Training epoch 98/100\n",
      "Training epoch 99/100\n",
      "Training epoch 100/100\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Key 'Artificial' not present in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 77\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_words): \n\u001b[1;32m     76\u001b[0m     last_word \u001b[38;5;241m=\u001b[39m sequence[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 77\u001b[0m     predicted_contexts \u001b[38;5;241m=\u001b[39m [predicted[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m predicted \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmost_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlast_word\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m     78\u001b[0m     next_word \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(predicted_contexts)\n\u001b[1;32m     79\u001b[0m     sequence\u001b[38;5;241m.\u001b[39mappend(next_word)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/gensim/models/keyedvectors.py:841\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    838\u001b[0m         weight[idx] \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    840\u001b[0m \u001b[38;5;66;03m# compute the weighted average of all keys\u001b[39;00m\n\u001b[0;32m--> 841\u001b[0m mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_mean_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    842\u001b[0m all_keys \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    843\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_index(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, _KEY_TYPES) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_index_for(key)\n\u001b[1;32m    844\u001b[0m ]\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(topn, \u001b[38;5;28mint\u001b[39m):\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/gensim/models/keyedvectors.py:518\u001b[0m, in \u001b[0;36mKeyedVectors.get_mean_vector\u001b[0;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[1;32m    516\u001b[0m         total_weight \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(weights[idx])\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ignore_missing:\n\u001b[0;32m--> 518\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not present in vocabulary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_weight \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    521\u001b[0m     mean \u001b[38;5;241m=\u001b[39m mean \u001b[38;5;241m/\u001b[39m total_weight\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'Artificial' not present in vocabulary\""
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import subprocess\n",
    "# subprocess.run([\"pip\", \"install\", \"wikipedia-api\"])\n",
    "import wikipediaapi\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Initializing the Wikipedia API\n",
    "wiki = wikipediaapi.Wikipedia('en', extract_format=wikipediaapi.ExtractFormat.WIKI, headers={'User-Agent': 'MyUserAgent/1.0'})\n",
    "\n",
    "#List wikipeida pages here\n",
    "page_titles = [\"Artificial intelligence\", \"Machine learning\", \"Data science\", \"Computers\", \"Programming languages\", \"Algorithms\", \"Robotics\", \"Computer vision\", \"Natural language processing\", \"Deep learning\",\n",
    "               \"Computer programming\", \"Neural networks\", \"Big data\", \"Cybersecurity\", \"Internet of things\", \"Cloud computing\", \"Operating systems\", \"Database management\", \"Software engineering\", \"Web development\",\n",
    "               \"Information technology\", \"Computer graphics\", \"Game development\", \"Bioinformatics\", \"Cryptography\", \"Computer architecture\", \"Network security\", \"Computer networks\", \"Data analysis\", \"Information retrieval\",\n",
    "               \"Parallel computing\", \"Quantum computing\", \"Virtual reality\", \"Augmented reality\", \"Machine translation\", \"Speech recognition\", \"Image processing\", \"Pattern recognition\", \"Human-computer interaction\", \"Computer-assisted design\",\n",
    "               \"Distributed computing\", \"Mobile computing\", \"Embedded systems\", \"Computer forensics\", \"Computer simulation\", \"Data mining\", \"Evolutionary computation\", \"Genetic algorithms\", \"Data visualization\"]\n",
    "\n",
    "# Fetch content from the above Wikipedia pages\n",
    "wiki_content = \"\"\n",
    "for title in page_titles:\n",
    "    page = wiki.page(title)\n",
    "    wiki_content += page.text\n",
    "\n",
    "print(\"Wikipedia Content Length:\", len(wiki_content))\n",
    "#tokenization\n",
    "dictionary_tokens = word_tokenize(wiki_content.lower())  # Convert to lowercase\n",
    "\n",
    "# Remove numbers and non-alphabetic tokens\n",
    "dictionary_tokens = [token for token in dictionary_tokens if token.isalpha()]\n",
    "\n",
    "\n",
    "#Reducing vocab size here\n",
    "vocab_size = 500000 # 500k words\n",
    "word_freq = nltk.FreqDist(dictionary_tokens)\n",
    "top_words = [word for word, _ in word_freq.most_common(vocab_size)]\n",
    "training_data = [word if word in top_words else \"<UNK>\" for word in dictionary_tokens]\n",
    "\n",
    "#Converting tokens into sentences\n",
    "sentences = [[word] for word in training_data]\n",
    "\n",
    "#Training the Word2Vec model with the following hyperparams\n",
    "vector_size = 20\n",
    "window = 2\n",
    "min_count = 1\n",
    "workers = 4\n",
    "sg = 1\n",
    "epochs = 100\n",
    "\n",
    "model = Word2Vec(vector_size=vector_size, window=window, min_count=min_count, workers=workers, sg=sg)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Training epoch {epoch + 1}/{epochs}\")\n",
    "    model.build_vocab(corpus_iterable=sentences, update=(epoch != 0))\n",
    "    model.train(corpus_iterable=sentences, total_examples=len(sentences), epochs=1)\n",
    "\n",
    "# Generate context-target pairs for evaluation\n",
    "def generate_context_target_pairs(tokens, window_size=2):\n",
    "    pairs = []\n",
    "    for i, target_word in enumerate(tokens):\n",
    "        if(i % 100000 == 0):\n",
    "            print(\"Processing token\", i, \"out of\", len(tokens))\n",
    "        for j in range(max(0, i - window_size), min(len(tokens), i + window_size + 1)):\n",
    "            if i != j:\n",
    "                context_word = tokens[j]\n",
    "                pairs.append((context_word, target_word))\n",
    "    return pairs\n",
    "\n",
    "#Example output\n",
    "random_word = \"Artificial\" #here we initialize with a random word. using page title for simplicity\n",
    "\n",
    "\n",
    "num_words = 20\n",
    "sequence = [random_word]\n",
    "for i in range(num_words): \n",
    "    last_word = sequence[-1]\n",
    "    predicted_contexts = [predicted[0] for predicted in model.wv.most_similar(last_word)]\n",
    "    next_word = np.random.choice(predicted_contexts)\n",
    "    sequence.append(next_word)\n",
    "\n",
    "#Printing this sequence separated with spaces\n",
    "print(\"Generated Sequence:\")\n",
    "print(\" \".join(sequence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sequence:\n",
      "artificial music deductions rogue animators propofol aborted stereoscopic nuevo trace vista pod viewer characterizations analyzer mistakes winter viral acls viral acls\n",
      "Generated Sequence:\n",
      "waveguides hospi cincom warren medium hostile marcian psychosis mayo dilemmas superstructure bitlocker precautions employment reassigned mayo floors manager spirits tend spirits\n",
      "Generated Sequence:\n",
      "exposure controller years increased briefly are hockey attempted intent gray spacewar illustrate transforms confirmatory critically joints assignments joints lossy alongside constructing\n",
      "Generated Sequence:\n",
      "zadeh terminating ut haiku beetles tobias sweetgreen outputs initial amongst surrogate amongst initial address leveled nikita brookman jwt metcalfe distributors computationalism\n",
      "Generated Sequence:\n",
      "fielding bethke threatens dynamics include mermin cables mermin boundaries relations transferring brake musée enterprising theoretically musée msa revitalized missiles bar sebastiaan\n",
      "Generated Sequence:\n",
      "thai academies another solves pentium deriving freeware multicloud depot underway baum exemption mandia returns amount debug cryptolinguistics deadlocks pace pb resourced\n",
      "Generated Sequence:\n",
      "subtotals nuevo lockheed alters gloveone hays pullela icv database zurich concealment criticized badly odometry illus badly odometry withstand badly withstand ebscohost\n",
      "Generated Sequence:\n",
      "likethat low heiman subsets diffie uniprot laughter acceptability descent ces visualizing cluttered queries imply queries deceased demonstrating passing smith johnson moritz\n",
      "Generated Sequence:\n",
      "piping opacity onedrive children hiding mosfet budgets infection decoding slated pauline soa brown anybots barrilleaux forums optimierung stuttgart measured stuttgart relate\n",
      "Generated Sequence:\n",
      "hygiene yuri memories globalization hotspots administers racial citing conditionals citing scholarly radius scholarly radius scholarly radius scholarly adhere eugene presumes dechter\n",
      "Generated Sequence:\n",
      "yoruba appreciation tiffincad focusing towards focusing digit freeman pioneering freeman parity worth dirichlet evaluating dave flippy wayfair sailing watchdog tokenization watchdog\n"
     ]
    }
   ],
   "source": [
    "# Example output\n",
    "def generate_sequence(random_word, num_words):\n",
    "    sequence = [random_word]\n",
    "    for i in range(num_words): \n",
    "        last_word = sequence[-1]\n",
    "        predicted_contexts = [predicted[0] for predicted in model.wv.most_similar(last_word)]\n",
    "        next_word = np.random.choice(predicted_contexts)\n",
    "        sequence.append(next_word)\n",
    "    return sequence\n",
    "\n",
    "#Generating a sequence of 20 words\n",
    "random_word = \"artificial\"\n",
    "sequence = generate_sequence(random_word, 20)\n",
    "print(\"Generated Sequence:\")\n",
    "print(\" \".join(sequence))\n",
    "\n",
    "for i in range(10):\n",
    "    random_word = np.random.choice(top_words)\n",
    "    sequence = generate_sequence(random_word, 20)\n",
    "    print(\"Generated Sequence:\")\n",
    "    print(\" \".join(sequence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
