{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of context windows: 272525\n",
      "Training epoch 1/5\n",
      "Training epoch 2/5\n",
      "Training epoch 3/5\n",
      "Training epoch 4/5\n",
      "Training epoch 5/5\n",
      "Generated Sequence:\n",
      "if var_1275 else var_5 var_64 var_1017 var_64 var_9313 var_13737 var_1434 var_33456 var_1434 var_28403 var_18088 var_1554 var_18883 var_18880 var_16909 var_29678 var_16226 var_29322 var_30840 var_18770 var_28338 var_29780 var_29291 var_15736 var_24992 var_29907 var_29953 var_28979 var_33455 var_14994 var_28338 var_1554 var_22006 var_19074 var_29578 var_30562 var_18483 var_28615 var_29832 var_28615 var_28979 var_28401 var_22024 var_23125 var_22024 var_33446 var_5331 var_28375 var_13482 var_16370 var_29807\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "import numpy as np\n",
    "import keyword\n",
    "import re\n",
    "\n",
    "# Define a function to extract Python files from a directory\n",
    "def extract_python_files(directory):\n",
    "    python_files = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".py\"):\n",
    "                python_files.append(os.path.join(root, file))\n",
    "    return python_files\n",
    "\n",
    "# Define a function to preprocess Python code and handle variable names\n",
    "def preprocess_python_code(file_paths):\n",
    "    code_corpus = \"\"\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            code = file.read()\n",
    "            code_corpus += code\n",
    "    return code_corpus\n",
    "\n",
    "# Tokenize the code while handling keywords, variables, and special characters\n",
    "def tokenize_code(code):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+|[^\\w\\s]+|\\s+')\n",
    "    tokens = tokenizer.tokenize(code)\n",
    "    \n",
    "    keywords = set(keyword.kwlist)\n",
    "    tokenized_code = []\n",
    "    variable_counter = {}\n",
    "    variable_id = 0\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in keywords:\n",
    "            tokenized_code.append(f\"<KEYWORD:{token}>\")\n",
    "        elif token.isidentifier() and not token.isnumeric():\n",
    "            if token not in variable_counter:\n",
    "                variable_counter[token] = f\"<VAR{variable_id}>\"\n",
    "                variable_id += 1\n",
    "            tokenized_code.append(variable_counter[token])\n",
    "        elif re.match(r'\\s+', token):  # Preserve whitespace\n",
    "            tokenized_code.append(token)\n",
    "        else:\n",
    "            tokenized_code.append(token)\n",
    "    \n",
    "    return tokenized_code\n",
    "\n",
    "# Extract Python files from the numpy directory\n",
    "directory = \"/Users/krishpatel/Desktop/Skipgram_Implementation/numpy\"\n",
    "numpy_files = extract_python_files(directory)\n",
    "\n",
    "# Preprocess Python code from numpy\n",
    "numpy_code = preprocess_python_code(numpy_files)\n",
    "\n",
    "# Tokenize the code\n",
    "dictionary_tokens = tokenize_code(numpy_code)\n",
    "\n",
    "# Reduce vocabulary size here\n",
    "vocab_size = 500000  # 500k words\n",
    "word_freq = nltk.FreqDist(dictionary_tokens)\n",
    "top_words = [word for word, _ in word_freq.most_common(vocab_size)]\n",
    "training_data = [word if word in top_words else \"<UNK>\" for word in dictionary_tokens]\n",
    "\n",
    "# Group tokens into sentences based on newline characters\n",
    "lines = ' '.join(training_data).split('\\n')\n",
    "\n",
    "# Create context windows of 2-3 sentences\n",
    "context_windows = []\n",
    "window_size = 3\n",
    "for i in range(len(lines) - window_size + 1):\n",
    "    window = lines[i:i + window_size]\n",
    "    context_windows.append(' '.join(window))\n",
    "\n",
    "print(\"Number of context windows:\", len(context_windows))\n",
    "\n",
    "# Split context windows into tokens\n",
    "sentences = [window.split() for window in context_windows]\n",
    "\n",
    "# Build vocabulary\n",
    "model = Word2Vec(vector_size=20, window=2, min_count=1, workers=4, sg=1)\n",
    "model.build_vocab(sentences)\n",
    "\n",
    "# Training the Word2Vec model with the following hyperparameters\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Training epoch {epoch + 1}/{epochs}\")\n",
    "    model.train(corpus_iterable=sentences, total_examples=len(sentences), epochs=1)\n",
    "\n",
    "# Provided sequence to start with\n",
    "starting_sequence = ['<KEYWORD:if>', '<VAR1275>', '=', '<KEYWORD:else>', '<VAR5>']\n",
    "\n",
    "# Generating a sequence starting with the provided tokens\n",
    "sequence = starting_sequence.copy()\n",
    "num_words_to_generate = 50\n",
    "\n",
    "for i in range(num_words_to_generate):\n",
    "    last_word = sequence[-1]\n",
    "    if last_word in model.wv:\n",
    "        predicted_contexts = [predicted[0] for predicted in model.wv.most_similar(last_word)]\n",
    "        next_word = np.random.choice(predicted_contexts)\n",
    "        sequence.append(next_word)\n",
    "    else:\n",
    "        print(f\"Word '{last_word}' not in vocabulary\")\n",
    "        break\n",
    "\n",
    "# Post-process generated sequence to format as code\n",
    "generated_code = ' '.join(sequence).replace('<KEYWORD:', '').replace('>', '').replace('<VAR', 'var').replace('var', 'var_')\n",
    "formatted_code = generated_code.replace(' <', '<').replace('> ', '>').replace('{ ', '{\\n').replace('} ', '}\\n').replace(' ;', ';\\n')\n",
    "\n",
    "print(\"Generated Sequence:\")\n",
    "print(formatted_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sequence:\n",
      "if var_1275 = var_1275 : var_10256 ^+` var_2489 var_19283 var_11604 var_11818 var_25948 var_13767 var_9473 var_13767 var_27810 var_10754 var_13767 var_27810 var_1764 var_3080 var_20281 var_29028 var_29678 var_16909 var_13941 var_15615 var_23864 var_9416 var_33456 var_33397 var_18088 var_22882 var_23072 var_24301 var_20850 var_30108 var_28872 var_16695 var_14498 var_28198 var_29094 var_28410 var_18088 var_23972 var_13767 var_18088 var_22006 var_18088 var_22006 var_28442 var_30150 var_19103 var_22986 var_23955\n",
      "First 10 words in the vocabulary:\n",
      "[',', '.', '(', '=', ')', '<VAR584>', '1', '0', ':', '<VAR106>']\n"
     ]
    }
   ],
   "source": [
    "# Provided sequence to start with\n",
    "starting_sequence = ['<KEYWORD:if>', '<VAR1275>', '=', '<VAR1275>', \":\"]\n",
    "\n",
    "# Generating a sequence starting with the provided tokens\n",
    "sequence = starting_sequence.copy()\n",
    "num_words_to_generate = 50\n",
    "\n",
    "for i in range(num_words_to_generate):\n",
    "    last_word = sequence[-1]\n",
    "    if last_word in model.wv:\n",
    "        predicted_contexts = [predicted[0] for predicted in model.wv.most_similar(last_word)]\n",
    "        next_word = np.random.choice(predicted_contexts)\n",
    "        sequence.append(next_word)\n",
    "    else:\n",
    "        print(f\"Word '{last_word}' not in vocabulary\")\n",
    "        break\n",
    "\n",
    "# Post-process generated sequence to format as code\n",
    "generated_code = ' '.join(sequence).replace('<KEYWORD:', '').replace('>', '').replace('<VAR', 'var').replace('var', 'var_')\n",
    "formatted_code = generated_code.replace(' <', '<').replace('> ', '>').replace('{ ', '{\\n').replace('} ', '}\\n').replace(' ;', ';\\n')\n",
    "\n",
    "print(\"Generated Sequence:\")\n",
    "print(formatted_code)\n",
    "print(\"First 10 words in the vocabulary:\")\n",
    "print(list(model.wv.index_to_key)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 words in the vocabulary:\n",
      "[' ', ',', '.', '(', '\\n        ', '=', '\\n    ', ')', '<VAR584>', '1', '0', ':', '<VAR106>', \"'\", '\\n            ', '<VAR1293>', '[', '\\n\\n    ', '2', '#', '<VAR7>', '-', '):', '<VAR1286>', '<VAR1278>', \"',\", '<KEYWORD:def>', '3', '\"', '<VAR396>', '<KEYWORD:is>', '\\n', '`', '<KEYWORD:in>', '),', '<KEYWORD:if>', '<VAR58>', '],', '<KEYWORD:for>', '\\n                ', '<VAR1275>', '\\n\\n        ', '<VAR5>', ']', '))', '5', '<KEYWORD:None>', '+', \"('\", '<VAR274>', '  ', '4', '([', '<KEYWORD:return>', '\",', '])', '>>>', '<KEYWORD:and>', '\"\"\"', '*', '<KEYWORD:not>', '==', '<KEYWORD:True>', '<KEYWORD:with>', '<VAR618>', '<VAR1325>', '``', '<VAR145>', '<KEYWORD:False>', '<VAR206>', '<VAR2144>', '\\n\\n', '\\n\\n\\n', '.,', '<VAR207>', '<VAR30>', '()', \"')\", '<VAR397>', '<KEYWORD:import>', '<VAR96>', '<KEYWORD:as>', \"['\", '<VAR7841>', \"':\", '10', '(\"', '<VAR205>', '<VAR880>', ').', '6', '<KEYWORD:from>', '/', '<VAR1573>', '<KEYWORD:or>', '<VAR298>', '\\n                    ', '<VAR93>', '<VAR177>', '\\\\', \"='\", '<VAR3565>', '%', '((', '<KEYWORD:else>', '<KEYWORD:assert>', '<VAR308>', '<VAR425>', '<VAR1101>', '<VAR174>', '<VAR7842>', '\")', '<VAR419>', '<VAR10895>', '@', '<VAR464>', '<KEYWORD:class>', '<VAR0>', '<VAR309>', '<VAR1966>', '<VAR1626>', '<VAR71>', '<VAR1364>', '<VAR406>', '{', '<VAR521>', '<VAR3357>', '<VAR31>', '<VAR1870>', '=\"', '7', '8', '<VAR1854>', '<VAR277>', '([[', '<VAR60>', '<VAR170>', '--------', '<VAR1481>', \"'),\", \"']\", '<VAR1096>', '<VAR115>', ']])', '<VAR435>', '<VAR572>', '**', '<VAR303>', '<VAR162>', '<VAR895>', '<VAR770>', '<VAR1003>', '9', '<VAR1349>', '<VAR1601>', '<VAR191>', '<VAR3674>', '<VAR250>', '<VAR131>', '<VAR1287>', '<KEYWORD:raise>', '<VAR266>', '<VAR377>', '<VAR786>', '::', '\\n                        ', '<VAR1267>', '<VAR189>', '<VAR88>', '<VAR14>', '<VAR399>', '<VAR313>', '<VAR438>', \"'],\", '\\n          ', '<VAR1251>', '<VAR892>', '\\n           ', '<VAR1066>', '}', '<VAR1288>', '<VAR6120>', '----------', '<VAR1007>', '<VAR728>', '<VAR1187>', '<VAR2047>', '<', '():', '>', '<VAR1147>', '<VAR4850>', '<VAR818>', '<VAR158>', '<VAR730>', '<VAR197>', '<VAR1756>', '<VAR22>', '`.', '<VAR1853>', '<VAR125>', '<VAR690>', '<VAR10443>', '<VAR1>', '<VAR67>', '[[', '<VAR569>', '<VAR1068>', '\\n                            ', '<VAR89>', '<VAR217>', '<VAR1227>', '<VAR144>', '..', '<VAR3>', '[-', '\\n\\n            ', '   ', '<VAR321>', '<VAR223>', '<VAR3454>', '<VAR2053>', '<VAR3365>', '<VAR760>', '<VAR7969>', '<VAR3457>', '<VAR523>', '<VAR1279>', '<VAR1869>', '<VAR10884>', '<VAR976>', '\\n             ', '(-', '<VAR33>', '<VAR1781>', '-------', '<VAR697>', '<VAR1377>', '=[', '<VAR2991>', '<VAR10262>', '<VAR79>', '<VAR286>', '<VAR634>', '[\"', '<VAR1408>', '<VAR1285>', '<KEYWORD:elif>', '<VAR400>', '<VAR133>', '<VAR765>', '12', '<VAR258>', '!=', '20', ')),', '<VAR83>', '11', '<VAR1738>', '<VAR116>', '<VAR2736>', '<VAR433>', '<VAR992>', '<VAR837>', ']],', '<VAR418>', '100', \"'))\", '<VAR704>', '<VAR1191>', '\\n                     ', '<VAR490>', '<VAR77>', '<VAR2385>', '<VAR725>', '<KEYWORD:try>', '<VAR39>', '`,', '<VAR3460>', '<VAR552>', '<KEYWORD:except>', '(),', ',),', '<VAR3426>', '())', '<VAR1489>', '<VAR1547>', '<VAR759>', '<VAR395>', '<VAR1347>', ')],', '[]', '<VAR772>', '|', '<VAR938>', '<VAR11878>', ';', '<VAR10896>', '].', '=(', '<VAR25321>', '<VAR1651>', '<VAR124>', '<VAR794>', '<VAR1176>', '<VAR1942>', '<VAR184>', '\"),', '<VAR2425>', '<VAR2801>', '...', '<VAR2116>', '<VAR4563>', '<VAR391>', '<VAR1987>', '\\n                      ', ')))', '\":', '<VAR585>', '<VAR571>', '<VAR342>', '<VAR2233>', '<VAR66>', '<VAR362>', '->', '<VAR1123>', '<VAR2052>', '<VAR2396>', ']),', '<VAR3295>', '<KEYWORD:lambda>', '<VAR3485>', '<VAR154>', '+=', '<VAR9818>', '<VAR1120>', '<VAR1284>', '<VAR1674>', '<VAR8267>', '<VAR574>', \"{'\", '<VAR1134>', '<VAR374>', '<VAR1423>', '<VAR1749>', '<VAR1062>', '.],', '<VAR9892>', '<VAR2147>', '<VAR673>', '<VAR10632>', '<VAR1691>', '<VAR544>', '<VAR647>', '<VAR234>', '<VAR691>', '<VAR675>', \"'):\", '<VAR431>', ']))', '    ', '<VAR2639>', '<VAR2689>', '<VAR998>', ')]', \"'])\", '<VAR1356>', '<VAR1480>', '<VAR345>', '\"]', '<VAR1112>', '<VAR984>', '<VAR546>', '<VAR10899>', '<VAR209>', '<VAR10885>', '-----', '<VAR1989>', '     ', '<VAR744>', '<VAR590>', '<VAR3459>', '<VAR10160>', '<VAR2730>', '16', '<VAR752>', '<KEYWORD:pass>', '<VAR300>', '15', '<VAR3438>', '<VAR1491>', '<VAR462>', '<VAR3548>', ',)', '<VAR25403>', '<VAR1188>', ')])', '<VAR262>', '<VAR702>', '<VAR23>', '<VAR1522>', '<VAR3427>', '<VAR1391>', '<VAR10934>', '<VAR1238>', '``,', '``.', '<VAR1280>', '<VAR272>', '<VAR1765>', '<VAR1699>', '<VAR11329>', '14', '<VAR1277>', '<VAR1831>', '<VAR11330>', '<VAR501>', '<VAR945>', '<VAR769>', '<VAR934>', '<VAR610>', \"'.\", '<VAR933>', '\\n      ', '<VAR3262>', ']*', '<VAR543>', '<VAR2351>', '<VAR10130>', '<VAR10912>', '<VAR1230>', ']]', '\"],', '<VAR123>', '<VAR1172>', '<VAR3279>', '(*', '<VAR2787>', '},', '<VAR1418>', '<VAR3711>', '1j', '\"):', '<VAR1137>', '<VAR7742>', '<VAR1919>', '<VAR4524>', '<VAR1346>', '<VAR427>', '<VAR2972>', \"'\\\\\", '<VAR1935>', '01', '<VAR11844>', '<VAR2117>', '<VAR2591>', ':`', '<VAR1158>', '<VAR4679>', '<VAR566>', '<VAR1363>', '<VAR1004>', '<VAR122>', '\\n               ', '<VAR3338>', '<VAR624>', '<VAR2437>', '<=', '<VAR2647>', \"=[('\", '<VAR3581>', '[:', '<VAR1925>', '<VAR888>', '<VAR80>', '<VAR410>', '<VAR9601>', '<VAR1283>', '<VAR1661>', '\\n                  ', '30', '<VAR3870>', '25', '<VAR200>', '=-', '<VAR550>', '\\\\\\\\', '<VAR245>', '<VAR1456>', '\\n                 ', '<VAR10879>', ']:', '\\n                             ', '<VAR10271>', '<VAR10447>', '<VAR7459>', '<VAR761>', '<VAR49>', '24', \"'<\", '<VAR1390>', '<VAR413>', '<VAR1272>', \"(['\", '<VAR1074>', '--', ':]', '<VAR1741>', '<VAR441>', '<VAR9706>', '<VAR11065>', '<VAR1116>', '<VAR1618>', '<VAR10619>', '13', '<VAR439>', '<VAR3263>', '().', '<VAR883>', \"]['\", '<VAR61>', '\\n       ', '([(', '<VAR4333>', '<VAR11269>', '<VAR9619>', '<VAR1844>', '<VAR2033>', '<VAR987>', '<VAR15056>', '<VAR139>', '32', '.])', '<VAR861>', '<VAR3881>', '<VAR4462>', '<VAR160>', '<VAR1452>', ',))', '<VAR2999>', '<VAR714>', '\\n                         ', '<VAR2430>', '64', '<VAR3528>', '<VAR2804>', '<VAR862>', '\\n              ', \"[('\", \"[['\", '<VAR2080>', '<VAR876>', '\\n                   ', '<VAR879>', '<VAR3334>', '[:,', '\\n         ', '<VAR9730>', '<VAR1696>', \"']:\", '1e', '<VAR755>', '<VAR2259>', '<VAR10321>', '<VAR1804>', '<VAR2852>', '//', '([-', '<VAR1504>', '<VAR28968>', '<VAR3065>', '<VAR3164>', '<VAR3375>', '[(', '<VAR775>', '$', '50', '<VAR1241>', '<VAR14203>', '<VAR3448>', '\\n                           ', '<VAR11195>', '<VAR3372>', '<VAR2339>', '<VAR1918>', '<VAR762>', '<VAR13006>', '<VAR3007>', '.+', '<VAR1996>', '<VAR3337>', '][', '        ', '<VAR1253>', '{}', '<VAR1274>', '<VAR1132>', '<VAR1754>', '\\n                       ', '<VAR11320>', '[],', ');', ')``', \"([('\", '<VAR3024>', '\\n   ', '<VAR22456>', '<VAR1257>', '<VAR2926>', ')):', '<VAR3419>', '>=', '<VAR301>', '<VAR1827>', '<VAR2753>', '<VAR3106>', '<VAR7325>', '<VAR16880>', '<KEYWORD:yield>', '<VAR511>', '<VAR1957>', '<VAR283>', '<VAR2272>', \"').\", '<VAR877>', '1000', '<VAR736>', '<VAR2567>', '<VAR2661>', '.)', '<VAR750>', '<VAR5256>', '<VAR9444>', '<VAR25011>', '<VAR3211>', '<VAR971>', '<VAR10946>', '<VAR2796>', '<VAR469>', '<VAR9564>', '<VAR6711>', '<VAR101>', '<VAR811>', '<VAR85>', '<VAR2039>', '<VAR3617>', '<VAR26836>', '<VAR3728>', '<VAR3620>', '<VAR1484>', '<VAR1420>', '<VAR1282>', '<VAR172>', \"'-\", '<VAR11231>', '\\n                          ', '<VAR25323>', '<VAR1515>', '<KEYWORD:continue>', '<VAR649>', '<VAR14222>', '<VAR9794>', '<VAR2735>', '<VAR537>', '<VAR1207>', '<VAR1149>', '<VAR1530>', '<VAR1913>', '<VAR731>', '<VAR871>', '\\n     ', '\"])', '<VAR9823>', '<VAR829>', '<VAR1385>', '<VAR7895>', '<VAR3315>', '<VAR1473>', '<VAR2780>', '<VAR2659>', '<VAR13344>', '<VAR2847>', '<VAR10887>', '\\n  ', \"')])\", '<VAR28719>', '<VAR2713>', '<VAR10282>', '<VAR1345>', '<VAR1425>', '<VAR2645>', '://', '<VAR7951>', '<VAR231>', '\\n                                ', '<VAR2838>', '<VAR2452>', '<VAR2703>', '<VAR1980>', '<VAR2165>', '<VAR2627>', '<VAR186>', '<VAR823>', '<VAR373>', '<VAR2525>', '      ', '<VAR10924>', '<VAR686>', '<VAR8400>', '<VAR1860>', '<VAR26308>', ']\",', '<VAR1695>', '<VAR16161>', '<VAR3355>', '<VAR24>', '17', '<VAR12>', '<VAR3139>', '<VAR2783>', '<VAR2939>', '<VAR1562>', ']).', '<VAR2149>', '<VAR2020>', '<VAR1767>', '<VAR3536>', \"'%\", '\"-', '<VAR1427>', '<VAR10521>', '<VAR3706>', '<VAR11034>', '<VAR1086>', '03', '=[[', '<VAR3278>', '<VAR2544>', '<VAR925>', '<VAR751>', '<VAR128>', '123', '<VAR164>', '<VAR600>', '<VAR1198>', '<VAR2692>', '<VAR3575>', '<VAR354>', '<VAR4222>', '<VAR25311>', '<VAR2378>', '<VAR11024>', '\\n                               ', '<VAR7986>', '<VAR25892>', '<VAR692>', '<VAR33358>', '<VAR2618>', '2011', '<VAR1106>', '<VAR2421>', '<VAR3428>', '<VAR2792>', '<VAR2985>', '<VAR2010>', '<VAR1152>', '<VAR2688>', '<VAR481>', '<KEYWORD:break>', '<VAR901>', '<VAR24006>', '<VAR885>', '<VAR10970>', '<VAR2789>', '<VAR3025>', \"''\", '<VAR2446>', '<VAR1186>', '<VAR2771>', '23', '\\n                              ', '<VAR3422>', '<VAR2063>', '<VAR8417>', '\"\"\")', '<VAR11194>', '<VAR314>', '<VAR3280>', '21', '<VAR1185>', '<VAR1262>', '<VAR25>', '<VAR1339>', '<VAR2644>', ')[', \"'].\", '<VAR78>', '<VAR10041>', '<VAR768>', '<VAR989>', '<VAR1812>', '00', '<VAR986>', '<VAR25329>', '<VAR2617>', '<VAR2394>', '&', '<KEYWORD:del>', '<VAR2725>', '<VAR11954>', '<VAR2936>', '<VAR143>', '<VAR2695>', '<VAR346>', '<VAR2612>', '<VAR4337>', '<VAR3547>', '<VAR2993>', '<VAR334>', '22', '<VAR801>', '<VAR699>', '*,', '<VAR857>', '<VAR3831>', '<VAR10182>', '<VAR7748>', '<VAR3694>', '<VAR3544>', '<VAR3549>', '<VAR4441>', '<VAR295>', '<VAR8810>', '!', '[::', '<VAR13766>', '<VAR2423>', '<VAR1460>', '<VAR11734>', '<VAR7664>', '<VAR2234>', '       ', '<VAR2981>', '<VAR3598>', '<KEYWORD:while>', '[:]', '<VAR273>', '<VAR1612>', '<VAR11531>', '<VAR2321>', '<VAR2441>', '99', '.\"\"\"', '<VAR5145>', '<VAR8192>', '2j', '<VAR1063>', '<VAR24247>', '<VAR1281>', '<VAR2092>', '<VAR1232>', '<VAR878>', '<VAR540>', '<VAR3839>', '<VAR2795>', '<VAR2590>', '<VAR10046>', '<VAR2651>', '<VAR3087>', '<VAR297>', \"]'))\", '<VAR228>', '<VAR1571>', '<VAR11598>', '<VAR1089>', '<VAR126>', '<VAR1782>', '<VAR2444>', '18', '<VAR2727>', '<VAR7007>', '<VAR107>', '<VAR715>', '<VAR4354>', '<VAR1319>', \"'},\", '<VAR7663>', '<VAR1487>', '<VAR863>', '<VAR4595>', '<VAR595>', '<VAR3901>', '\".', '<VAR29090>', '<VAR379>', '<VAR2180>', ')).', '<VAR2935>', \"]',\", '<VAR10739>', '<VAR2655>', '02', '<VAR13954>', '<VAR13577>', '<VAR2454>', '<VAR1598>', '<VAR961>', '<VAR534>', '<VAR10510>', \"'',\", '<VAR316>', '<VAR2850>', '<VAR6314>', '<VAR3001>', '<VAR466>', '<VAR32617>', '<VAR2077>', '<VAR2621>', '<VAR4349>', '<VAR1814>', '<VAR2919>', '<VAR1076>', '<VAR9302>', '<VAR2864>', '<VAR476>', '<VAR3208>', '<VAR2413>', '<VAR1465>', '<VAR1669>', '[:-', '<VAR3325>', '<VAR1906>', '<VAR97>', '<VAR2373>', '([\"', '<VAR11530>', '<VAR1354>', '<VAR12298>', '<VAR530>', '\"{', '<VAR10140>', '<VAR1340>', '<VAR1011>', '<VAR15985>', '<VAR376>', '[...]', '<VAR138>', '<VAR1890>', '<VAR10623>', '<VAR3402>', '<VAR1183>', '<VAR727>', '<VAR14058>', '<VAR8191>', '31', '\"\\\\', '<VAR2895>', '<VAR7638>', '<VAR302>', '<VAR1729>', '<VAR1190>', '<VAR3410>', '<VAR1750>', '<VAR1139>', '<VAR3601>', '.\")', '<VAR211>', '<VAR33527>', '<VAR10314>', '<VAR448>', '<VAR3258>', '<VAR402>', \"'][\", '<VAR1610>', \"'>\", '<VAR3333>', '<VAR1981>', '<VAR3050>', '.]])', ')*', '<VAR13870>', '<VAR2746>', '<VAR1632>', '<VAR4414>', '<VAR3464>', '60', '<VAR34250>', '<VAR26221>', '<VAR4895>', \"']])\", '\\n\\n                ', '<VAR1517>', '<VAR27>', '<VAR65>', '<VAR9582>', '<VAR271>', '<VAR32387>', '<VAR2582>', '<VAR17124>', '<VAR1446>']\n",
      "Generated Sequence:\n",
      "<KEYWORD:if> 4108474671 <VAR32179> 4108474671 <KEYWORD:if> <VAR23717> <VAR25233> '>'\\ <VAR25233> <VAR24903> <VAR5078> 4520525295346629 <VAR17969> <VAR24317> <VAR23452> <VAR34041> 15817 ('@' <VAR1819> ('@' 15817\n"
     ]
    }
   ],
   "source": [
    "print(\"First 10 words in the vocabulary:\")\n",
    "print(list(model.wv.index_to_key)[:1000])\n",
    "\n",
    "random_word = '<KEYWORD:if>'\n",
    "num_words = 20\n",
    "sequence = [random_word]\n",
    "for i in range(num_words):\n",
    "    last_word = sequence[-1]\n",
    "    predicted_contexts = [predicted[0] for predicted in model.wv.most_similar(last_word)]\n",
    "    next_word = np.random.choice(predicted_contexts)\n",
    "    sequence.append(next_word)\n",
    "\n",
    "# Printing this sequence separated with spaces\n",
    "print(\"Generated Sequence:\")\n",
    "print(\" \".join(sequence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 2187441\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/backend.py\", line 4979, in <listcomp>\n        inputs, [inp[0] for inp in flatted_inputs]\n\n    ValueError: Exception encountered when calling layer 'lstm_1' (type LSTM).\n    \n    slice index 0 of dimension 0 out of bounds. for '{{node strided_slice_1}} = StridedSlice[Index=DT_INT32, T=DT_FLOAT, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1](transpose, strided_slice_1/stack, strided_slice_1/stack_1, strided_slice_1/stack_2)' with input shapes: [0,?,20], [1], [1], [1] and with computed input tensors: input[1] = <0>, input[2] = <1>, input[3] = <1>.\n    \n    Call arguments received by layer 'lstm_1' (type LSTM):\n      • inputs=tf.Tensor(shape=(None, 0, 20), dtype=float32)\n      • mask=None\n      • training=True\n      • initial_state=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 78\u001b[0m\n\u001b[1;32m     76\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(padded_sequences[:, \u001b[38;5;241m1\u001b[39m:], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     77\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(y, (y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# Ensure y has 3 dimensions\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m \u001b[43mmodel_lstm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m random_word \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mif\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     80\u001b[0m num_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/jl/_k9n1dsd7rg1x3mrfs7130y40000gn/T/__autograph_generated_filek2ptfccx.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/backend.py\", line 4979, in <listcomp>\n        inputs, [inp[0] for inp in flatted_inputs]\n\n    ValueError: Exception encountered when calling layer 'lstm_1' (type LSTM).\n    \n    slice index 0 of dimension 0 out of bounds. for '{{node strided_slice_1}} = StridedSlice[Index=DT_INT32, T=DT_FLOAT, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1](transpose, strided_slice_1/stack, strided_slice_1/stack_1, strided_slice_1/stack_2)' with input shapes: [0,?,20], [1], [1], [1] and with computed input tensors: input[1] = <0>, input[2] = <1>, input[3] = <1>.\n    \n    Call arguments received by layer 'lstm_1' (type LSTM):\n      • inputs=tf.Tensor(shape=(None, 0, 20), dtype=float32)\n      • mask=None\n      • training=True\n      • initial_state=None\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Define a function to extract Python files from a directory\n",
    "def extract_python_files():\n",
    "    python_files = []\n",
    "    directory = \"/Users/krishpatel/Desktop/Skipgram_Implementation/numpy\"\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".py\"):\n",
    "                python_files.append(os.path.join(root, file))\n",
    "    return python_files\n",
    "\n",
    "# Define a function to preprocess Python code and handle variable names\n",
    "def preprocess_python_code(file_paths):\n",
    "    code_corpus = \"\"\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            code = file.read()\n",
    "            code_corpus += code\n",
    "    return code_corpus\n",
    "\n",
    "# Extract Python files from the numpy directory\n",
    "numpy_files = extract_python_files()\n",
    "\n",
    "# Preprocess Python code from numpy\n",
    "numpy_code = preprocess_python_code(numpy_files)\n",
    "\n",
    "# Tokenize the code while keeping special characters\n",
    "tokenizer = RegexpTokenizer(r'[\\w\\s]+|\\S')\n",
    "\n",
    "# Tokenize the code\n",
    "dictionary_tokens = tokenizer.tokenize(numpy_code)\n",
    "\n",
    "# Reduce vocabulary size here\n",
    "vocab_size = 500000  # 500k words\n",
    "word_freq = nltk.FreqDist(dictionary_tokens)\n",
    "top_words = [word for word, _ in word_freq.most_common(vocab_size)]\n",
    "training_data = [word if word in top_words else \"<UNK>\" for word in dictionary_tokens]\n",
    "\n",
    "# Convert tokens into sentences\n",
    "sentences = [[word] for word in training_data]\n",
    "print(\"Number of sentences:\", len(sentences))\n",
    "\n",
    "# Build vocabulary\n",
    "model = Word2Vec(vector_size=20, window=2, min_count=1, workers=4, sg=1)\n",
    "model.build_vocab(sentences)\n",
    "\n",
    "# Convert words to integers\n",
    "word2idx = {word: idx for idx, word in enumerate(model.wv.index_to_key)}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "sequences = [[word2idx[word] for word in sent if word in word2idx] for sent in sentences]\n",
    "\n",
    "# Padding sequences\n",
    "max_seq_length = max(len(seq) for seq in sequences)\n",
    "padded_sequences = np.array([seq + [0]*(max_seq_length-len(seq)) for seq in sequences])\n",
    "\n",
    "# Define the LSTM model\n",
    "embedding_dim = 20\n",
    "model_lstm = Sequential([\n",
    "    Embedding(input_dim=len(word2idx), output_dim=embedding_dim, input_length=max_seq_length),\n",
    "    LSTM(128),\n",
    "    Dense(len(word2idx), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_lstm.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "X = padded_sequences[:, :-1]\n",
    "y = np.expand_dims(padded_sequences[:, 1:], axis=-1)\n",
    "y = np.reshape(y, (y.shape[0], y.shape[1], 1))  # Ensure y has 3 dimensions\n",
    "model_lstm.fit(X, y, epochs=10, batch_size=128)\n",
    "random_word = 'if'\n",
    "num_words = 20\n",
    "sequence = [random_word]\n",
    "for i in range(num_words): \n",
    "    last_word = sequence[-1]\n",
    "    predicted_contexts = model.wv.most_similar(last_word)  # Adjust this line based on the model used\n",
    "    next_word = np.random.choice(predicted_contexts)\n",
    "    sequence.append(next_word)\n",
    "\n",
    "# Printing this sequence separated with spaces\n",
    "print(\"Generated Sequence:\")\n",
    "print(\" \".join(sequence))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 2187441\n",
      "Training epoch 1/5\n",
      "Training epoch 2/5\n",
      "Training epoch 3/5\n",
      "Training epoch 4/5\n",
      "Training epoch 5/5\n",
      "Generated Sequence:\n",
      "if \n",
      "            f77flags  \n",
      "subtract  \n",
      "        return fd \n",
      "            version   for a in dividend \n",
      "            version  NpyIter_GetInnerFixedStrideArray \n",
      "        sub_class  dtype\n",
      "        assert_array_equal 9989\n",
      "    fname  \n",
      "    s_medium  9989\n",
      "    fname  \n",
      "                case NPY_SHORT \n",
      "            ediff1d \n",
      "                case NPY_SHORT sysconfig  state\n",
      "        int_2  \n",
      "        if arg is None \n",
      "                if not success or args  for a in dividend\n"
     ]
    }
   ],
   "source": [
    "#intial code: \n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define a function to extract Python files from a directory\n",
    "def extract_python_files():\n",
    "    python_files = []\n",
    "    directory = \"/Users/krishpatel/Desktop/Skipgram_Implementation/numpy\"\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".py\"):\n",
    "                python_files.append(os.path.join(root, file))\n",
    "    return python_files\n",
    "\n",
    "# Define a function to preprocess Python code and handle variable names\n",
    "def preprocess_python_code(file_paths):\n",
    "    code_corpus = \"\"\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            code = file.read()\n",
    "            code_corpus += code\n",
    "    return code_corpus\n",
    "\n",
    "# Extract Python files from the numpy directory\n",
    "numpy_files = extract_python_files()\n",
    "\n",
    "# Preprocess Python code from numpy\n",
    "numpy_code = preprocess_python_code(numpy_files)\n",
    "\n",
    "# Tokenize the code while keeping special characters\n",
    "tokenizer = RegexpTokenizer(r'[\\w\\s]+|\\S')\n",
    "\n",
    "# Tokenize the code\n",
    "dictionary_tokens = tokenizer.tokenize(numpy_code)\n",
    "\n",
    "# Reduce vocabulary size here\n",
    "vocab_size = 500000  # 500k words\n",
    "word_freq = nltk.FreqDist(dictionary_tokens)\n",
    "top_words = [word for word, _ in word_freq.most_common(vocab_size)]\n",
    "training_data = [word if word in top_words else \"<UNK>\" for word in dictionary_tokens]\n",
    "\n",
    "# Convert tokens into sentences\n",
    "sentences = [[word] for word in training_data]\n",
    "print(\"Number of sentences:\", len(sentences))\n",
    "\n",
    "# Build vocabulary\n",
    "model = Word2Vec(vector_size=20, window=2, min_count=1, workers=4, sg=1)\n",
    "model.build_vocab(sentences)\n",
    "\n",
    "# Training the Word2Vec model with the following hyperparameters\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Training epoch {epoch + 1}/{epochs}\")\n",
    "    model.train(corpus_iterable=sentences, total_examples=len(sentences), epochs=1)\n",
    "\n",
    "# Example output\n",
    "random_word = 'if'\n",
    "num_words = 20\n",
    "sequence = [random_word]\n",
    "for i in range(num_words): \n",
    "    last_word = sequence[-1]\n",
    "    predicted_contexts = [predicted[0] for predicted in model.wv.most_similar(last_word)]\n",
    "    next_word = np.random.choice(predicted_contexts)\n",
    "    sequence.append(next_word)\n",
    "\n",
    "# Printing this sequence separated with spaces\n",
    "print(\"Generated Sequence:\")\n",
    "print(\" \".join(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'if' not present in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_words): \n\u001b[1;32m      5\u001b[0m     last_word \u001b[38;5;241m=\u001b[39m sequence[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m----> 6\u001b[0m     predicted_contexts \u001b[38;5;241m=\u001b[39m [predicted[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m predicted \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmost_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlast_word\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m      7\u001b[0m     next_word \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(predicted_contexts)\n\u001b[1;32m      8\u001b[0m     sequence\u001b[38;5;241m.\u001b[39mappend(next_word)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/gensim/models/keyedvectors.py:841\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    838\u001b[0m         weight[idx] \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    840\u001b[0m \u001b[38;5;66;03m# compute the weighted average of all keys\u001b[39;00m\n\u001b[0;32m--> 841\u001b[0m mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_mean_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    842\u001b[0m all_keys \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    843\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_index(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, _KEY_TYPES) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_index_for(key)\n\u001b[1;32m    844\u001b[0m ]\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(topn, \u001b[38;5;28mint\u001b[39m):\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/gensim/models/keyedvectors.py:518\u001b[0m, in \u001b[0;36mKeyedVectors.get_mean_vector\u001b[0;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[1;32m    516\u001b[0m         total_weight \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(weights[idx])\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ignore_missing:\n\u001b[0;32m--> 518\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not present in vocabulary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_weight \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    521\u001b[0m     mean \u001b[38;5;241m=\u001b[39m mean \u001b[38;5;241m/\u001b[39m total_weight\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'if' not present in vocabulary\""
     ]
    }
   ],
   "source": [
    "random_word = 'if'\n",
    "num_words = 20\n",
    "sequence = [random_word]\n",
    "for i in range(num_words): \n",
    "    last_word = sequence[-1]\n",
    "    predicted_contexts = [predicted[0] for predicted in model.wv.most_similar(last_word)]\n",
    "    next_word = np.random.choice(predicted_contexts)\n",
    "    sequence.append(next_word)\n",
    "\n",
    "# Printing this sequence separated with spaces\n",
    "print(\"Generated Sequence:\")\n",
    "print(\" \".join(sequence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
